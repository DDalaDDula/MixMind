{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = ['Liz', '최고주가들만 파트 또 많이 넣었네ㅋㅋㅋㅋ리즈, 레이는 그냥 최고주가가 스스로 되는 수 밖에 없으려나ㅋㅋㅋㅋㅋ스타쉽 징하네 진짜ㅋㅋㅋㅋㅋ', 'Liz deserves more lines..!! why is starship unfair to Liz??!!', 'STARSHİP!!! GİVE MORE LİNE FOR OUR LİZ!!!!', 'what does  it mean?', \"Can someone inform me what a 'kitsch' is?😂\", 'IVE愛してる\\U0001faf6', 'リズ覚醒しててすき！！', 'What a great song', 'Full Album on April 10th!!!!!', \"A music video literally never angered me more. I love these girls and it's no fault of them, but what the label is doing to this group is so shameful. Poor Liz got no screen time or lines in the song. I repeatedly saw the same girls over and over in this video while she was hidden. They couldn't try any harder to hide her, It's horrible. Someone pick this girl up under a new group so she can be allowed to shine! I'm so tired of certain girls in this group being so praised by the label while Liz is treated like a background character. It's one thing for fandoms to have favorites, but for a label to and make it so obvious is messed up. It's not even like she's bad, her voice is wonderful, she's beautiful inside and out, and her performance skills are great. She knows how to do her job! To be treated like a wall because you don't fit your labels beauty standard is disgusting. They should drop her from the label if they don't like her instead of suppressing her so she can go elsewhere. The song is great, really cool video, and the girls look and sound beautiful, but it's not fair that one of their members is treated so poorly every freaking song/performance.\", '41.038', 'めっちゃ良い^^', 'so unfair, Liz deserves more😔', '리즈 파트 무슨 일이야... 장난하지 말고;;', '리즈는 왜 한번만 불러요?', '원영이 보컬이 듣기 편안한 소리구나!\\n비음빼니까 너무 좋다. 이서도 비음을 좀 빼야될듯', '처음엔  뭐지  했는데\\n계속 들으니까 좋아지는 노래', 'Rei slayed her parts', '41,015,111 M congrats IVE!!!', 'Keren', 'Very nice song', '好き', 'holy bang bang bang❤', 'They just dropped the concept photos!', 'Ni YouTube bener\" ya 😢 udh cape\" streaming malah di apus mulu view nya 💔', 'starship distribute lines based on popularity, liz got blonde hair and opening lines as soon as she debuted meaning that starship wanted to popularize her, but obviously she took that for granted', 'I much prefer this group to New Jeans. IVE does music right.', 'yt don´t delete ive views??', 'épica', 'ame esta canción', \"Amazing as always\\nI'm so into this song! Let's go 45M\", '이서 너무 예쁘다', 'OOTD BITHC OOTD!!!!!!!', 'Nos estan eliminando vistas ,por eso no avanzamos \\n      P*TO  YOUTUBE (con todas las ganas de ofender , pero a la ves no XD)', 'visuaaaalll love', '나만 뮤비 구리게 느껴지나? 러브 다이브 같은 거 보고 싶은데', '매일 듣고 있어요\\n노래 너무 좋다', \"We can't deny wonyoung's visual is a wow factor\", '0:07 ポニテウォニョンビジュ良すぎ', 'They knew Liz would slay if she had more lines', '잘 보고 들었습니다.', 'うわーまた来たよ神曲', 'yknow what this sounds lie? soty', 'wake up in the morning', 'I dunno but this song such a downgrade for me... Eleven and Love Dive still in my heart, After Like is 50:50', 'Starship do it so good I know what do u do like that for liz😂', \"788k like >40m>ads<flop 😅😅😅they're only popular in Korean digital music\", 'Liz got me', 'Ya estaba en 41M youtube...😡', 'Están eliminando vistas :/', \"40M again!! YouTube! Stop it's 41M but YouTube keeps deleting it\", \"The music is quite addicted and the scene is stunning too,however,the transition of the mv seems a little bit orderless......It's kind of confusion.\", 'lets go dive!', 'felt bad for liz', 'The chorus', 'Is Liz leaving Ive?', 'love this song!!!😍😍😍', 'Epic track.... all IVE tracks are epic', 'Liz ❤', 'woonyoung !!!! <3', '今年もIVEの年にするぞー！', '今回のmvも最高', 'ya me da miedo que todas las canciones de ive sean buenas', 'The fact that youtube kept deleting our views... we could have easily passed 45 mil by now 🙄', '41.023', 'REI ROSITA NUNCA T VAYAS', 'go', 'Finalmente este grupito demostrando su pobre popularidad con este MV llamado chish que sacaron.\\nUna vergueza de numeros.\\nFracaso.', '장원영 개이쁘다!!!!!!!!!!!❤❤❤', '이 노래 리즈 음색 듣고,깜짝 놀랬었는데,제 딸이 한파트라고 하더군요.\\n그 얘기듣고,너무 놀랬어요.\\n한파트라도  처음들었던 제 귀엔 제일 꽂혔네요~', '그리고 매크로 바이럴 댓글이라 지들 새끼 아이디만 답글으루달수 있게 조작해놓음 그러다보니 죄다 영어임 아이브가 그렇게 해외에서 인기가 있었다고', 'go dive!', 'Can we have a 100 hours loop of Liz part please', '이거봐라 쿠키 달고 아이브 홍보하러 다니는거....이런게 바로 바이럴 주작이지 아이브=주작그룹', '21th PERFECT ALL KILL\\nKitsch – Update : #1 MelOn (=) #1 FLO (=) #1 Genie (+1) #1 Bugs (=) #1 Vibe (=) #1 Youtube (=) #1 Spotify (=) #1 Apple (=) 🔥🤭', '41 M !!! LETS GO 50 M 🐦', \"PAK! Well deserved. I just can't stop myself from watching this 100x times a day lol\", '노래대박이다❤❤❤❤❤', '20th PERFECT ALL KILL\\nKitsch – Update : #1 MelOn (=) #1 FLO (=) #1 Genie (+1) #1 Bugs (=)\\n😎☝️😎☝️😎☝️😎☝️😎☝️😎☝️', \"_From highly-anticipated returns to can't-miss debut releases, check out 15 albums dropping this April from Linkin Park, IVE, Rae Sremmurd, and more_\\n\\n- Grammy Award News\", '노래 중독성있다', 'NewJeans win IVE😡😡😡😡😡😡😡😡😡😡💚💚💚💚😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣😡🤣🤣😡😡🤣😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😘😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡', 'Kakao/Melon/IVE 🎉🎉🎉🎉🎉every time IVE can be No.1🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉', '2:53 is my favorite part in this mv', 'did liz nasty', 'NewJeans win IVE😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😡😘😘😘😡😡😡😡😡😡😡😡😡😡😡😡', '조회수 4천1백만 눈앞...!', '뮤비색감이랑 의상 진짜 잘 뽑았다\\n스타쉽이 배운 변태', '메인 타이틀에는 리즈 파트가 많길......😮\\u200d💨', '메보가 메보가 아닌 그룹', \"20th PERFECT ALL KILL\\n 'Kitsch' – 10AM KST: \\n\\n#1 MelOn (=) \\n#1 FLO (=) \\n#1 Genie (=) \\n#1 Bugs (=) \\n\\nMelOn ULs: 398,081 (+1,163)\", 'malarda', '리즈 파트가 너무 없어요ㅠㅜ', '20th PERFECT ALL KILL\\nKitsch – Update : #1 MelOn (=) #1 FLO (=) #1 Genie (+1) #1 Bugs (=)\\n😎☝️😎☝️😎☝️😎☝️😎☝️😎☝️', \"Liz's part is so short but sweet , looking forward to the full new album. I hope everyone has their proper line distribution.\", 'YUH YUH YUH YUH YUH YUH YUH', 'It looks like Liz is not part of the group anymore.', 'I LOVE REI', 'THEIR CONFIDENCE >>>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>리뷰</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>최고주가들만 파트 또 많이 넣었네ㅋㅋㅋㅋ리즈, 레이는 그냥 최고주가가 스스로 되는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Liz deserves more lines..!! why is starship un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STARSHİP!!! GİVE MORE LİNE FOR OUR LİZ!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what does  it mean?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Liz's part is so short but sweet , looking for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>YUH YUH YUH YUH YUH YUH YUH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>It looks like Liz is not part of the group any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I LOVE REI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>THEIR CONFIDENCE &gt;&gt;&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   리뷰\n",
       "0                                                 Liz\n",
       "1   최고주가들만 파트 또 많이 넣었네ㅋㅋㅋㅋ리즈, 레이는 그냥 최고주가가 스스로 되는 ...\n",
       "2   Liz deserves more lines..!! why is starship un...\n",
       "3          STARSHİP!!! GİVE MORE LİNE FOR OUR LİZ!!!!\n",
       "4                                 what does  it mean?\n",
       "..                                                ...\n",
       "95  Liz's part is so short but sweet , looking for...\n",
       "96                        YUH YUH YUH YUH YUH YUH YUH\n",
       "97  It looks like Liz is not part of the group any...\n",
       "98                                         I LOVE REI\n",
       "99                               THEIR CONFIDENCE >>>\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(my_list)\n",
    "my_df.rename(columns={0:'리뷰'}, inplace=True)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>사랑</td>\n",
       "      <td>나도 알아 날 괴롭히는 것도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>사랑</td>\n",
       "      <td>고민은 항상 날 무너트리네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>사랑</td>\n",
       "      <td>세상에 맘껏 날 쏟아부어도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>사랑</td>\n",
       "      <td>남아있는 건 나 하나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>사랑</td>\n",
       "      <td>이젠 나를 보면 찡해</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6345</th>\n",
       "      <td>분노</td>\n",
       "      <td>어깨 잡고 흔들고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6346</th>\n",
       "      <td>분노</td>\n",
       "      <td>미친 듯이 욕하고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6347</th>\n",
       "      <td>분노</td>\n",
       "      <td>찔러 상처 내봐도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6348</th>\n",
       "      <td>분노</td>\n",
       "      <td>사랑했어 정말</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6349</th>\n",
       "      <td>분노</td>\n",
       "      <td>아니 Fuck you 너나 꺼져</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6350 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     emotion              lyric\n",
       "0         사랑    나도 알아 날 괴롭히는 것도\n",
       "1         사랑     고민은 항상 날 무너트리네\n",
       "2         사랑     세상에 맘껏 날 쏟아부어도\n",
       "3         사랑        남아있는 건 나 하나\n",
       "4         사랑        이젠 나를 보면 찡해\n",
       "...      ...                ...\n",
       "6345      분노          어깨 잡고 흔들고\n",
       "6346      분노          미친 듯이 욕하고\n",
       "6347      분노          찔러 상처 내봐도\n",
       "6348      분노            사랑했어 정말\n",
       "6349      분노  아니 Fuck you 너나 꺼져\n",
       "\n",
       "[6350 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정있는 가사 데이터를 불러와보자\n",
    "\n",
    "emo_data = pd.read_csv('/home/cshoon036/MixMind/mm_bert/korean/emo_data.csv')\n",
    "emo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['사랑', '행복', '신나는', '열정', '슬픔', '그리움', '외로움', '두려움', '분노'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_data['emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(emotion):\n",
    "    if emotion == 0:\n",
    "        return None\n",
    "    # 긍정적인 감정은 1로 치환\n",
    "    elif (emotion == \"사랑\" or emotion == \"행복\" or emotion == \"신나는\" or emotion == \"열정\"):\n",
    "        return 1\n",
    "    # 부정적인 감정은 0으로 치환\n",
    "    elif (emotion == \"슬픔\" or emotion == \"그리움\" or emotion == \"외로움\" or emotion == \"두려움\", emotion == \"분노\"):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긍/부정 치환\n",
    "emo_data['sentiment'] = emo_data['emotion'].map(lambda x : get_sentiment(x))\n",
    "\n",
    "emo_data = emo_data[(emo_data['sentiment'] == 1) | (emo_data['sentiment'] == 0)]\n",
    "emo_data['sentiment'] = emo_data['sentiment'].astype('int64')\n",
    "emo_data = emo_data[['lyric','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>나도 알아 날 괴롭히는 것도</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>고민은 항상 날 무너트리네</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>세상에 맘껏 날 쏟아부어도</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>남아있는 건 나 하나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이젠 나를 보면 찡해</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6345</th>\n",
       "      <td>어깨 잡고 흔들고</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6346</th>\n",
       "      <td>미친 듯이 욕하고</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6347</th>\n",
       "      <td>찔러 상처 내봐도</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6348</th>\n",
       "      <td>사랑했어 정말</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6349</th>\n",
       "      <td>아니 Fuck you 너나 꺼져</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6350 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  lyric  sentiment\n",
       "0       나도 알아 날 괴롭히는 것도          1\n",
       "1        고민은 항상 날 무너트리네          1\n",
       "2        세상에 맘껏 날 쏟아부어도          1\n",
       "3           남아있는 건 나 하나          1\n",
       "4           이젠 나를 보면 찡해          1\n",
       "...                 ...        ...\n",
       "6345          어깨 잡고 흔들고          0\n",
       "6346          미친 듯이 욕하고          0\n",
       "6347          찔러 상처 내봐도          0\n",
       "6348            사랑했어 정말          0\n",
       "6349  아니 Fuck you 너나 꺼져          0\n",
       "\n",
       "[6350 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 08:56:53.628768: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "loading file vocab.txt from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tokenizer_config.json\n",
      "loading configuration file config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "  0%|          | 0/6350 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 6350/6350 [00:00<00:00, 6377.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sents: 6350, # labels: 6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from emotion_analysis import bert_tokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sent, train_label in tqdm(zip(emo_data[\"lyric\"], emo_data[\"sentiment\"]), total=len(emo_data)):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        train_data_labels.append(train_label)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass\n",
    "\n",
    "train_lyric_input_ids = np.array(input_ids, dtype=int)\n",
    "train_lyric_attention_masks = np.array(attention_masks, dtype=int)\n",
    "train_lyric_type_ids = np.array(token_type_ids, dtype=int)\n",
    "train_lyric_inputs = (train_lyric_input_ids, train_lyric_attention_masks, train_lyric_type_ids)\n",
    "\n",
    "train_data_labels = np.asarray(train_data_labels, dtype=np.int32) #레이블 토크나이징 리스트\n",
    "\n",
    "print(\"# sents: {}, # labels: {}\".format(len(train_lyric_input_ids), len(train_data_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 25\n",
    "VALID_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tokenizer_config.json\n",
      "loading configuration file config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  8888 36553 10892  9959 14871  8985  9294 70162 15184 12692 77884\n",
      "   102     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[CLS] 고민은 항상 날 무너트리네 [SEP] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "input_id = train_lyric_input_ids[1]\n",
    "attention_mask = train_lyric_attention_masks[1]\n",
    "token_type_id = train_lyric_type_ids[1]\n",
    "\n",
    "print(input_id)\n",
    "print(attention_mask)\n",
    "print(token_type_id)\n",
    "print(tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 08:57:10.678780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.704111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.704483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.705285: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 08:57:10.705750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.706001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.706188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:11.377301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:11.377574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:11.377827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:11.377996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14618 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0\n",
      "loading configuration file config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file tf_model.h5 from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tf_model.h5\n",
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from emotion_analysis import TFBertClassifier\n",
    "\n",
    "sentiment_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:121: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "# 총 batch size * 4 epoch = 2344 * 4\n",
    "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*2, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "sentiment_model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tf2_bert_sentiment -- Folder already exists \n",
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318/318 [==============================] - ETA: 0s - loss: 0.6393 - accuracy: 0.6331\n",
      "Epoch 1: val_loss improved from inf to 0.91802, saving model to ./tf2_bert_sentiment/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n",
      "2023-04-05 09:01:00.546163: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at save_restore_v2_ops.cc:138 : RESOURCE_EXHAUSTED: tf2_bert_sentiment/best_model/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate1257553640276108254; No space left on device\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__SaveV2_dtypes_618_device_/job:localhost/replica:0/task:0/device:CPU:0}} tf2_bert_sentiment/best_model/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate1257553640276108254; No space left on device [Op:SaveV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m cp_callback \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     21\u001b[0m     checkpoint_path, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m ,save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m , save_weight_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[39m# 학습과 eval 시작\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m history \u001b[39m=\u001b[39m sentiment_model\u001b[39m.\u001b[39;49mfit(train_lyric_inputs, train_data_labels, epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m     25\u001b[0m                     validation_split \u001b[39m=\u001b[39;49m VALID_SPLIT, callbacks\u001b[39m=\u001b[39;49m[earlystop_callback, cp_callback])\n\u001b[1;32m     27\u001b[0m \u001b[39m#steps_for_epoch\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(history\u001b[39m.\u001b[39mhistory)\n",
      "File \u001b[0;32m/opt/conda/envs/mixmind/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/envs/mixmind/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__SaveV2_dtypes_618_device_/job:localhost/replica:0/task:0/device:CPU:0}} tf2_bert_sentiment/best_model/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate1257553640276108254; No space left on device [Op:SaveV2]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_name = \"tf2_bert_sentiment\"\n",
    "\n",
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=3)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
    "\n",
    "checkpoint_path = os.path.join(\"./\", model_name, 'best_model')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss', verbose=1, mode='min' ,save_best_only=True , save_weight_only=True)\n",
    "\n",
    "# 학습과 eval 시작\n",
    "history = sentiment_model.fit(train_lyric_inputs, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
    "\n",
    "#steps_for_epoch\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in my_df.itertuples():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_analysis import sentence_convert_data, sentence_evaluation_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixmind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
