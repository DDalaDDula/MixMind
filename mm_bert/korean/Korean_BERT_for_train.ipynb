{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitsch = ['Liz', 'ÏµúÍ≥†Ï£ºÍ∞ÄÎì§Îßå ÌååÌä∏ Îòê ÎßéÏù¥ ÎÑ£ÏóàÎÑ§„Öã„Öã„Öã„ÖãÎ¶¨Ï¶à, Î†àÏù¥Îäî Í∑∏ÎÉ• ÏµúÍ≥†Ï£ºÍ∞ÄÍ∞Ä Ïä§Ïä§Î°ú ÎêòÎäî Ïàò Î∞ñÏóê ÏóÜÏúºÎ†§ÎÇò„Öã„Öã„Öã„Öã„ÖãÏä§ÌÉÄÏâΩ ÏßïÌïòÎÑ§ ÏßÑÏßú„Öã„Öã„Öã„Öã„Öã', 'Liz deserves more lines..!! why is starship unfair to Liz??!!', 'STARSHƒ∞P!!! Gƒ∞VE MORE Lƒ∞NE FOR OUR Lƒ∞Z!!!!', 'what does  it mean?', \"Can someone inform me what a 'kitsch' is?üòÇ\", 'IVEÊÑõ„Åó„Å¶„Çã\\U0001faf6', '„É™„Ç∫Ë¶öÈÜí„Åó„Å¶„Å¶„Åô„ÅçÔºÅÔºÅ', 'What a great song', 'Full Album on April 10th!!!!!', \"A music video literally never angered me more. I love these girls and it's no fault of them, but what the label is doing to this group is so shameful. Poor Liz got no screen time or lines in the song. I repeatedly saw the same girls over and over in this video while she was hidden. They couldn't try any harder to hide her, It's horrible. Someone pick this girl up under a new group so she can be allowed to shine! I'm so tired of certain girls in this group being so praised by the label while Liz is treated like a background character. It's one thing for fandoms to have favorites, but for a label to and make it so obvious is messed up. It's not even like she's bad, her voice is wonderful, she's beautiful inside and out, and her performance skills are great. She knows how to do her job! To be treated like a wall because you don't fit your labels beauty standard is disgusting. They should drop her from the label if they don't like her instead of suppressing her so she can go elsewhere. The song is great, really cool video, and the girls look and sound beautiful, but it's not fair that one of their members is treated so poorly every freaking song/performance.\", '41.038', '„ÇÅ„Å£„Å°„ÇÉËâØ„ÅÑ^^', 'so unfair, Liz deserves moreüòî', 'Î¶¨Ï¶à ÌååÌä∏ Î¨¥Ïä® ÏùºÏù¥Ïïº... Ïû•ÎÇúÌïòÏßÄ ÎßêÍ≥†;;', 'Î¶¨Ï¶àÎäî Ïôú ÌïúÎ≤àÎßå Î∂àÎü¨Ïöî?', 'ÏõêÏòÅÏù¥ Î≥¥Ïª¨Ïù¥ Îì£Í∏∞ Ìé∏ÏïàÌïú ÏÜåÎ¶¨Íµ¨ÎÇò!\\nÎπÑÏùåÎπºÎãàÍπå ÎÑàÎ¨¥ Ï¢ãÎã§. Ïù¥ÏÑúÎèÑ ÎπÑÏùåÏùÑ Ï¢Ä ÎπºÏïºÎê†ÎìØ', 'Ï≤òÏùåÏóî  Î≠êÏßÄ  ÌñàÎäîÎç∞\\nÍ≥ÑÏÜç Îì§ÏúºÎãàÍπå Ï¢ãÏïÑÏßÄÎäî ÎÖ∏Îûò', 'Rei slayed her parts', '41,015,111 M congrats IVE!!!', 'Keren', 'Very nice song', 'Â•Ω„Åç', 'holy bang bang bang‚ù§', 'They just dropped the concept photos!', 'Ni YouTube bener\" ya üò¢ udh cape\" streaming malah di apus mulu view nya üíî', 'starship distribute lines based on popularity, liz got blonde hair and opening lines as soon as she debuted meaning that starship wanted to popularize her, but obviously she took that for granted', 'I much prefer this group to New Jeans. IVE does music right.', 'yt don¬¥t delete ive views??', '√©pica', 'ame esta canci√≥n', \"Amazing as always\\nI'm so into this song! Let's go 45M\", 'Ïù¥ÏÑú ÎÑàÎ¨¥ ÏòàÏÅòÎã§', 'OOTD BITHC OOTD!!!!!!!', 'Nos estan eliminando vistas ,por eso no avanzamos \\n      P*TO  YOUTUBE (con todas las ganas de ofender , pero a la ves no XD)', 'visuaaaalll love', 'ÎÇòÎßå ÎÆ§ÎπÑ Íµ¨Î¶¨Í≤å ÎäêÍª¥ÏßÄÎÇò? Îü¨Î∏å Îã§Ïù¥Î∏å Í∞ôÏùÄ Í±∞ Î≥¥Í≥† Ïã∂ÏùÄÎç∞', 'Îß§Ïùº Îì£Í≥† ÏûàÏñ¥Ïöî\\nÎÖ∏Îûò ÎÑàÎ¨¥ Ï¢ãÎã§', \"We can't deny wonyoung's visual is a wow factor\", '0:07 „Éù„Éã„ÉÜ„Ç¶„Ç©„Éã„Éß„É≥„Éì„Ç∏„É•ËâØ„Åô„Åé', 'They knew Liz would slay if she had more lines', 'Ïûò Î≥¥Í≥† Îì§ÏóàÏäµÎãàÎã§.', '„ÅÜ„Çè„Éº„Åæ„ÅüÊù•„Åü„ÇàÁ•ûÊõ≤', 'yknow what this sounds lie? soty', 'wake up in the morning', 'I dunno but this song such a downgrade for me... Eleven and Love Dive still in my heart, After Like is 50:50', 'Starship do it so good I know what do u do like that for lizüòÇ', \"788k like >40m>ads<flop üòÖüòÖüòÖthey're only popular in Korean digital music\", 'Liz got me', 'Ya estaba en 41M youtube...üò°', 'Est√°n eliminando vistas :/', \"40M again!! YouTube! Stop it's 41M but YouTube keeps deleting it\", \"The music is quite addicted and the scene is stunning too,however,the transition of the mv seems a little bit orderless......It's kind of confusion.\", 'lets go dive!', 'felt bad for liz', 'The chorus', 'Is Liz leaving Ive?', 'love this song!!!üòçüòçüòç', 'Epic track.... all IVE tracks are epic', 'Liz ‚ù§', 'woonyoung !!!! <3', '‰ªäÂπ¥„ÇÇIVE„ÅÆÂπ¥„Å´„Åô„Çã„Åû„ÉºÔºÅ', '‰ªäÂõû„ÅÆmv„ÇÇÊúÄÈ´ò', 'ya me da miedo que todas las canciones de ive sean buenas', 'The fact that youtube kept deleting our views... we could have easily passed 45 mil by now üôÑ', '41.023', 'REI ROSITA NUNCA T VAYAS', 'go', 'Finalmente este grupito demostrando su pobre popularidad con este MV llamado chish que sacaron.\\nUna vergueza de numeros.\\nFracaso.', 'Ïû•ÏõêÏòÅ Í∞úÏù¥ÏÅòÎã§!!!!!!!!!!!‚ù§‚ù§‚ù§', 'Ïù¥ ÎÖ∏Îûò Î¶¨Ï¶à ÏùåÏÉâ Îì£Í≥†,ÍπúÏßù ÎÜÄÎû¨ÏóàÎäîÎç∞,Ï†ú Îî∏Ïù¥ ÌïúÌååÌä∏ÎùºÍ≥† ÌïòÎçîÍµ∞Ïöî.\\nÍ∑∏ ÏñòÍ∏∞Îì£Í≥†,ÎÑàÎ¨¥ ÎÜÄÎû¨Ïñ¥Ïöî.\\nÌïúÌååÌä∏ÎùºÎèÑ  Ï≤òÏùåÎì§ÏóàÎçò Ï†ú Í∑ÄÏóî Ï†úÏùº ÍΩÇÌòîÎÑ§Ïöî~', 'Í∑∏Î¶¨Í≥† Îß§ÌÅ¨Î°ú Î∞îÏù¥Îü¥ ÎåìÍ∏ÄÏù¥Îùº ÏßÄÎì§ ÏÉàÎÅº ÏïÑÏù¥ÎîîÎßå ÎãµÍ∏ÄÏúºÎ£®Îã¨Ïàò ÏûàÍ≤å Ï°∞ÏûëÌï¥ÎÜìÏùå Í∑∏Îü¨Îã§Î≥¥Îãà Ï£ÑÎã§ ÏòÅÏñ¥ÏûÑ ÏïÑÏù¥Î∏åÍ∞Ä Í∑∏Î†áÍ≤å Ìï¥Ïô∏ÏóêÏÑú Ïù∏Í∏∞Í∞Ä ÏûàÏóàÎã§Í≥†', 'go dive!', 'Can we have a 100 hours loop of Liz part please', 'Ïù¥Í±∞Î¥êÎùº Ïø†ÌÇ§ Îã¨Í≥† ÏïÑÏù¥Î∏å ÌôçÎ≥¥ÌïòÎü¨ Îã§ÎãàÎäîÍ±∞....Ïù¥Îü∞Í≤å Î∞îÎ°ú Î∞îÏù¥Îü¥ Ï£ºÏûëÏù¥ÏßÄ ÏïÑÏù¥Î∏å=Ï£ºÏûëÍ∑∏Î£π', '21th PERFECT ALL KILL\\nKitsch ‚Äì Update : #1 MelOn (=) #1 FLO (=) #1 Genie (+1) #1 Bugs (=) #1 Vibe (=) #1 Youtube (=) #1 Spotify (=) #1 Apple (=) üî•ü§≠', '41 M !!! LETS GO 50 M üê¶', \"PAK! Well deserved. I just can't stop myself from watching this 100x times a day lol\", 'ÎÖ∏ÎûòÎåÄÎ∞ïÏù¥Îã§‚ù§‚ù§‚ù§‚ù§‚ù§', '20th PERFECT ALL KILL\\nKitsch ‚Äì Update : #1 MelOn (=) #1 FLO (=) #1 Genie (+1) #1 Bugs (=)\\nüòé‚òùÔ∏èüòé‚òùÔ∏èüòé‚òùÔ∏èüòé‚òùÔ∏èüòé‚òùÔ∏èüòé‚òùÔ∏è', \"_From highly-anticipated returns to can't-miss debut releases, check out 15 albums dropping this April from Linkin Park, IVE, Rae Sremmurd, and more_\\n\\n- Grammy Award News\", 'ÎÖ∏Îûò Ï§ëÎèÖÏÑ±ÏûàÎã§', 'NewJeans win IVEüò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üíöüíöüíöüíöüò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£üò°ü§£ü§£üò°üò°ü§£üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üòòüò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°', 'Kakao/Melon/IVE üéâüéâüéâüéâüéâevery time IVE can be No.1üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ', '2:53 is my favorite part in this mv', 'did liz nasty', 'NewJeans win IVEüò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üòòüòòüòòüò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°', 'Ï°∞ÌöåÏàò 4Ï≤ú1Î∞±Îßå ÎààÏïû...!', 'ÎÆ§ÎπÑÏÉâÍ∞êÏù¥Îûë ÏùòÏÉÅ ÏßÑÏßú Ïûò ÎΩëÏïòÎã§\\nÏä§ÌÉÄÏâΩÏù¥ Î∞∞Ïö¥ Î≥ÄÌÉú', 'Î©îÏù∏ ÌÉÄÏù¥ÌãÄÏóêÎäî Î¶¨Ï¶à ÌååÌä∏Í∞Ä ÎßéÍ∏∏......üòÆ\\u200düí®', 'Î©îÎ≥¥Í∞Ä Î©îÎ≥¥Í∞Ä ÏïÑÎãå Í∑∏Î£π', \"20th PERFECT ALL KILL\\n 'Kitsch' ‚Äì 10AM KST: \\n\\n#1 MelOn (=) \\n#1 FLO (=) \\n#1 Genie (=) \\n#1 Bugs (=) \\n\\nMelOn ULs: 398,081 (+1,163)\", 'malarda', 'Î¶¨Ï¶à ÌååÌä∏Í∞Ä ÎÑàÎ¨¥ ÏóÜÏñ¥Ïöî„Ö†„Öú', '20th PERFECT ALL KILL\\nKitsch ‚Äì Update : #1 MelOn (=) #1 FLO (=) #1 Genie (+1) #1 Bugs (=)\\nüòé‚òùÔ∏èüòé‚òùÔ∏èüòé‚òùÔ∏èüòé‚òùÔ∏èüòé‚òùÔ∏èüòé‚òùÔ∏è', \"Liz's part is so short but sweet , looking forward to the full new album. I hope everyone has their proper line distribution.\", 'YUH YUH YUH YUH YUH YUH YUH', 'It looks like Liz is not part of the group anymore.', 'I LOVE REI', 'THEIR CONFIDENCE >>>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning ÏúÑÌïú review Îç∞Ïù¥ÌÑ∞\n",
    "# train = pd.read_csv('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt', header=0, delimiter='\\t' ,quoting=3)[50000:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train['document'].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.dropna(inplace=True)\n",
    "# train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mm_bert.korean.kor_emotion_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = '/home/cshoon036/MixMind/mm_bert/korean/vocab_9class_500.csv'\n",
    "stopwords_path = '/home/cshoon036/MixMind/mm_bert/korean/stopwords.txt'\n",
    "\n",
    "vocab = load_vocab(vocab_path, stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "VALID_SPLIT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data[:1000] # for test\n",
    "'''\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sent, train_label in tqdm(zip(train[\"document\"], train[\"label\"]), total=len(train)):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        train_data_labels.append(train_label)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass\n",
    "\n",
    "train_sentence_input_ids = np.array(input_ids, dtype=int)\n",
    "train_sentence_attention_masks = np.array(attention_masks, dtype=int)\n",
    "train_sentence_type_ids = np.array(token_type_ids, dtype=int)\n",
    "train_sentence_inputs = (train_sentence_input_ids, train_sentence_attention_masks, train_sentence_type_ids)\n",
    "\n",
    "train_data_labels = np.asarray(train_data_labels, dtype=np.int32) #Î†àÏù¥Î∏î ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Î¶¨Ïä§Ìä∏\n",
    "\n",
    "print(\"# sents: {}, # labels: {}\".format(len(train_sentence_input_ids), len(train_data_labels)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   101   8915 118657   9519 101743  67313   9110  39420 119081  10739\n",
      "   9004  32537   8932  91837  10530   8982  77884  48549   9405  61250\n",
      "  10892   9331  28396  11261   9113  30873  33305   9654 118940  10530\n",
      "  19105   9708 119235  11261   9309  34951  14523  14523    100    102\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "[CLS] ÍµøÍµø ÏïÑÎùºÏùò ÎßàÏßÄÎßâ Îí∑Î™®ÏäµÏù¥ ÎÑàÎ¨¥ Í∏∞ÏñµÏóê ÎÇòÎÑ§Ïöî ÏÇ¨ÎûåÏùÄ Î∞±ÌîÑÎ°ú ÎìúÎü¨ÎÇú ÏûòÎ™ªÏóêÎßå ÏßÑÏßúÎ°ú ÎØ∏ÏïàÌï¥Ìï¥ [UNK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "input_id = train_sentence_input_ids[1]\n",
    "attention_mask = train_sentence_attention_masks[1]\n",
    "token_type_id = train_sentence_type_ids[1]\n",
    "\n",
    "print(input_id)\n",
    "print(attention_mask)\n",
    "print(token_type_id)\n",
    "print(tokenizer.decode(input_id))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectified Adam ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÇ¨Ïö©\n",
    "#!pip install tensorflow_addons\n",
    "import tensorflow_addons as tfa\n",
    "# Ï¥ù batch size * 4 epoch = 2344 * 4\n",
    "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "sentiment_model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tf2_bert_sentiment -- Folder create complete \n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2344/2344 [==============================] - ETA: 0s - loss: 0.3645 - accuracy: 0.8401\n",
      "Epoch 1: val_loss improved from inf to 0.34750, saving model to ./tf2_bert_sentiment/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tf2_bert_sentiment/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tf2_bert_sentiment/best_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2344/2344 [==============================] - 765s 313ms/step - loss: 0.3645 - accuracy: 0.8401 - val_loss: 0.3475 - val_accuracy: 0.8533\n",
      "Epoch 2/50\n",
      "2344/2344 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8692\n",
      "Epoch 2: val_loss did not improve from 0.34750\n",
      "2344/2344 [==============================] - 566s 241ms/step - loss: 0.3052 - accuracy: 0.8692 - val_loss: 0.3599 - val_accuracy: 0.8523\n",
      "Epoch 3/50\n",
      "2344/2344 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.8957\n",
      "Epoch 3: val_loss did not improve from 0.34750\n",
      "2344/2344 [==============================] - 566s 241ms/step - loss: 0.2557 - accuracy: 0.8957 - val_loss: 0.4080 - val_accuracy: 0.8516\n",
      "Epoch 4/50\n",
      "2344/2344 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9137\n",
      "Epoch 4: val_loss did not improve from 0.34750\n",
      "2344/2344 [==============================] - 565s 241ms/step - loss: 0.2183 - accuracy: 0.9137 - val_loss: 0.4300 - val_accuracy: 0.8505\n",
      "{'loss': [0.3645321726799011, 0.30520737171173096, 0.25568264722824097, 0.21827280521392822], 'accuracy': [0.8400757312774658, 0.8691698312759399, 0.8957305550575256, 0.9137043356895447], 'val_loss': [0.3475014865398407, 0.35993504524230957, 0.40804436802864075, 0.43000277876853943], 'val_accuracy': [0.8532800078392029, 0.8523200154304504, 0.8515999913215637, 0.8504800200462341]}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_sentiment\"\n",
    "\n",
    "# overfittingÏùÑ ÎßâÍ∏∞ ÏúÑÌïú ealrystop Ï∂îÍ∞Ä\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=3)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1Î≤à Ïù¥ÏÉÅ ÏÉÅÏäπÏù¥ ÏóÜÏúºÎ©¥ Ï¢ÖÎ£å)\\\n",
    "\n",
    "checkpoint_path = os.path.join(\"./\", model_name, 'best_model')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss', verbose=1, mode='min' ,save_best_only=True , save_weight_only=True)\n",
    "\n",
    "# ÌïôÏäµÍ≥º eval ÏãúÏûë\n",
    "history = sentiment_model.fit(train_sentence_inputs, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
    "\n",
    "#steps_for_epoch\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï∂îÍ∞ÄÌïôÏäµÌï†Îïå Î∂àÎü¨ÏòµÎãàÎã§...!\n",
    "# sentiment_model.load_weights('korean_binary_sentiment_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.save_weights('korean_binary_sentiment_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt', header=0, delimiter='\\t' ,quoting=3)[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dropna(inplace=True)\n",
    "test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19998 [00:00<?, ?it/s]/opt/conda/envs/mixmind/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19998/19998 [00:06<00:00, 3128.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sents: 19998, # labels: 19998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "test_token_type_ids = []\n",
    "test_data_labels = []\n",
    "\n",
    "for test_sent, test_label in tqdm(zip(test[\"document\"], test[\"label\"]), total=len(test)):\n",
    "    try:\n",
    "        test_input_id, test_attention_mask, test_token_type_id = bert_tokenizer(test_sent)\n",
    "        \n",
    "        test_input_ids.append(test_input_id)\n",
    "        test_attention_masks.append(test_attention_mask)\n",
    "        test_token_type_ids.append(test_token_type_id)\n",
    "        test_data_labels.append(test_label)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(test_sent)\n",
    "        pass\n",
    "\n",
    "test_sentence_input_ids = np.array(test_input_ids, dtype=int)\n",
    "test_sentence_attention_masks = np.array(test_attention_masks, dtype=int)\n",
    "test_sentence_type_ids = np.array(test_token_type_ids, dtype=int)\n",
    "test_sentence_inputs = (test_sentence_input_ids, test_sentence_attention_masks, test_sentence_type_ids)\n",
    "\n",
    "test_data_labels = np.asarray(test_data_labels, dtype=np.int32) #Î†àÏù¥Î∏î ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Î¶¨Ïä§Ìä∏\n",
    "\n",
    "print(\"# sents: {}, # labels: {}\".format(len(test_sentence_input_ids), len(test_data_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_raw = sentiment_model.predict(test_sentence_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19998"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = np.round(predicted_raw, 0)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84      9921\n",
      "           1       0.83      0.88      0.86     10077\n",
      "\n",
      "    accuracy                           0.85     19998\n",
      "   macro avg       0.85      0.85      0.85     19998\n",
      "weighted avg       0.85      0.85      0.85     19998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['label'], y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÏµúÍ≥†Ï£ºÍ∞ÄÎì§Îßå ÌååÌä∏ Îòê ÎßéÏù¥ ÎÑ£ÏóàÎÑ§„Öã„Öã„Öã„ÖãÎ¶¨Ï¶à Î†àÏù¥Îäî Í∑∏ÎÉ• ÏµúÍ≥†Ï£ºÍ∞ÄÍ∞Ä Ïä§Ïä§Î°ú ÎêòÎäî Ïàò Î∞ñÏóê ÏóÜÏúºÎ†§ÎÇò„Öã„Öã„Öã„Öã„ÖãÏä§ÌÉÄÏâΩ ÏßïÌïòÎÑ§ ÏßÑÏßú„Öã„Öã„Öã„Öã„Öã',\n",
       " 'Î¶¨Ï¶à ÌååÌä∏ Î¨¥Ïä® ÏùºÏù¥Ïïº Ïû•ÎÇúÌïòÏßÄ ÎßêÍ≥†',\n",
       " 'Î¶¨Ï¶àÎäî Ïôú ÌïúÎ≤àÎßå Î∂àÎü¨Ïöî',\n",
       " 'ÏõêÏòÅÏù¥ Î≥¥Ïª¨Ïù¥ Îì£Í∏∞ Ìé∏ÏïàÌïú ÏÜåÎ¶¨Íµ¨ÎÇò ÎπÑÏùåÎπºÎãàÍπå ÎÑàÎ¨¥ Ï¢ãÎã§ Ïù¥ÏÑúÎèÑ ÎπÑÏùåÏùÑ Ï¢Ä ÎπºÏïºÎê†ÎìØ',\n",
       " 'Ï≤òÏùåÏóî Î≠êÏßÄ ÌñàÎäîÎç∞ Í≥ÑÏÜç Îì§ÏúºÎãàÍπå Ï¢ãÏïÑÏßÄÎäî ÎÖ∏Îûò',\n",
       " 'Ïù¥ÏÑú ÎÑàÎ¨¥ ÏòàÏÅòÎã§',\n",
       " 'ÎÇòÎßå ÎÆ§ÎπÑ Íµ¨Î¶¨Í≤å ÎäêÍª¥ÏßÄÎÇò Îü¨Î∏å Îã§Ïù¥Î∏å Í∞ôÏùÄ Í±∞ Î≥¥Í≥† Ïã∂ÏùÄÎç∞',\n",
       " 'Îß§Ïùº Îì£Í≥† ÏûàÏñ¥Ïöî ÎÖ∏Îûò ÎÑàÎ¨¥ Ï¢ãÎã§',\n",
       " 'Ïûò Î≥¥Í≥† Îì§ÏóàÏäµÎãàÎã§',\n",
       " 'Ïû•ÏõêÏòÅ Í∞úÏù¥ÏÅòÎã§',\n",
       " 'Ïù¥ ÎÖ∏Îûò Î¶¨Ï¶à ÏùåÏÉâ Îì£Í≥† ÍπúÏßù ÎÜÄÎû¨ÏóàÎäîÎç∞ Ï†ú Îî∏Ïù¥ ÌïúÌååÌä∏ÎùºÍ≥† ÌïòÎçîÍµ∞Ïöî Í∑∏ ÏñòÍ∏∞Îì£Í≥† ÎÑàÎ¨¥ ÎÜÄÎû¨Ïñ¥Ïöî ÌïúÌååÌä∏ÎùºÎèÑ Ï≤òÏùåÎì§ÏóàÎçò Ï†ú Í∑ÄÏóî Ï†úÏùº ÍΩÇÌòîÎÑ§Ïöî',\n",
       " 'Í∑∏Î¶¨Í≥† Îß§ÌÅ¨Î°ú Î∞îÏù¥Îü¥ ÎåìÍ∏ÄÏù¥Îùº ÏßÄÎì§ ÏÉàÎÅº ÏïÑÏù¥ÎîîÎßå ÎãµÍ∏ÄÏúºÎ£®Îã¨Ïàò ÏûàÍ≤å Ï°∞ÏûëÌï¥ÎÜìÏùå Í∑∏Îü¨Îã§Î≥¥Îãà Ï£ÑÎã§ ÏòÅÏñ¥ÏûÑ ÏïÑÏù¥Î∏åÍ∞Ä Í∑∏Î†áÍ≤å Ìï¥Ïô∏ÏóêÏÑú Ïù∏Í∏∞Í∞Ä ÏûàÏóàÎã§Í≥†',\n",
       " 'Ïù¥Í±∞Î¥êÎùº Ïø†ÌÇ§ Îã¨Í≥† ÏïÑÏù¥Î∏å ÌôçÎ≥¥ÌïòÎü¨ Îã§ÎãàÎäîÍ±∞ Ïù¥Îü∞Í≤å Î∞îÎ°ú Î∞îÏù¥Îü¥ Ï£ºÏûëÏù¥ÏßÄ ÏïÑÏù¥Î∏å Ï£ºÏûëÍ∑∏Î£π',\n",
       " 'ÎÖ∏ÎûòÎåÄÎ∞ïÏù¥Îã§',\n",
       " 'ÎÖ∏Îûò Ï§ëÎèÖÏÑ±ÏûàÎã§',\n",
       " 'Ï°∞ÌöåÏàò Ï≤ú Î∞±Îßå ÎààÏïû',\n",
       " 'ÎÆ§ÎπÑÏÉâÍ∞êÏù¥Îûë ÏùòÏÉÅ ÏßÑÏßú Ïûò ÎΩëÏïòÎã§ Ïä§ÌÉÄÏâΩÏù¥ Î∞∞Ïö¥ Î≥ÄÌÉú',\n",
       " 'Î©îÏù∏ ÌÉÄÏù¥ÌãÄÏóêÎäî Î¶¨Ï¶à ÌååÌä∏Í∞Ä ÎßéÍ∏∏',\n",
       " 'Î©îÎ≥¥Í∞Ä Î©îÎ≥¥Í∞Ä ÏïÑÎãå Í∑∏Î£π',\n",
       " 'Î¶¨Ï¶à ÌååÌä∏Í∞Ä ÎÑàÎ¨¥ ÏóÜÏñ¥Ïöî']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kitsch_kor = []\n",
    "kitsch_kor_join = []\n",
    "\n",
    "p = re.compile('[„Ñ±-„ÖéÍ∞Ä-Ìû£]+')\n",
    "\n",
    "for i in kitsch:\n",
    "    if p.findall(i):\n",
    "        kitsch_kor.append(p.findall(i))\n",
    "\n",
    "for i in kitsch_kor:\n",
    "    kitsch_kor_join.append(\" \".join(i))\n",
    "\n",
    "kitsch_kor_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Î¶¨Î∑∞ Í∞úÏàò -> ÎÇòÏ§ëÏóê ÏΩîÎìú Î∞îÎÄú\n",
    "tot = len(kitsch_kor_join)\n",
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "song_emotion = {'5359' : 0, '5370': 0, '5361' : 0, '5363' : 0, '5364' : 0, '5365' : 0, '5366' : 0, '5367': 0, '5368' : 0, '5369': 0}\n",
    "\n",
    "for i in kitsch_kor_join:\n",
    "    temp = hybrid_emotion_export_persent(i, sentiment_model, vocab)\n",
    "    for j in temp.keys():\n",
    "        song_emotion[j] += temp[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5359': 0.23961749999999996,\n",
       " '5370': 0.11249749999999999,\n",
       " '5361': 0.13446249999999998,\n",
       " '5363': 0.09137300000000001,\n",
       " '5364': 0.06183599999999999,\n",
       " '5365': 0.076861,\n",
       " '5366': 0.044972500000000006,\n",
       " '5367': 0.0409075,\n",
       " '5368': 0.0474715,\n",
       " '5369': 0.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kitsch ÌïúÍ∏Ä Î¶¨Î∑∞Ïùò Ï¥ù Í∞êÏ†ï ÌèâÍ∑†\n",
    "for key, val in song_emotion.items():\n",
    "    song_emotion[key] = val/tot\n",
    "\n",
    "song_emotion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixmind",
   "language": "python",
   "name": "mixmind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
